{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4496910f-3f53-417a-9089-5e666ed4e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section1 Load Dataset\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1a35e83-0881-4ac4-8557-9556d4087e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = pd.read_csv(\"C:/Users/Muhammad Mitkar/Desktop/LSN/merged_datasetokis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23dd0c20-2d90-4c48-b1f1-8a787816f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32337 entries, 0 to 32336\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Text      32337 non-null  object\n",
      " 1   Language  32337 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 505.4+ KB\n",
      "None\n",
      "                                                Text  Language\n",
      "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
      "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
      "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
      "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
      "4  de spons behoort tot het geslacht haliclona en...     Dutch\n"
     ]
    }
   ],
   "source": [
    "print(merged_dataset.info())\n",
    "print(merged_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4087e688-e38e-484e-a3ff-9b741dd9f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Text Language\n",
      "count                                               32337    32337\n",
      "unique                                              32126       30\n",
      "top     haec commentatio automatice praeparata res ast...  English\n",
      "freq                                                   48     2385\n"
     ]
    }
   ],
   "source": [
    "print(merged_dataset.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abc540-53eb-4d7d-9986-17f024b99d68",
   "metadata": {},
   "source": [
    "# EDA AND Text Processing on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fefbd99c-bdfb-4766-a2b1-4e4d6ab77b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.dropna(inplace=True)\n",
    "merged_dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e15d191-4eb1-4e0b-97c0-863a25c6d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "# Apply text cleaning to the 'Text' column\n",
    "merged_dataset['Text'] = merged_dataset['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6de85a19-e541-4487-9224-9d0558436ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the language labels\n",
    "label_encoder = LabelEncoder()\n",
    "merged_dataset['Language_Encoded'] = label_encoder.fit_transform(merged_dataset['Language'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867e8f7-5bc6-4f38-9aa7-1d5b451c447c",
   "metadata": {},
   "source": [
    "# Generating NLP based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c53422a-261b-4b6f-ac95-e3d0913ed5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and labels\n",
    "texts = merged_dataset['Text'].values\n",
    "labels = merged_dataset['Language_Encoded'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd9a07a2-b579-481e-929e-94384ec2c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=100, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=100, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc295d4-3ea2-49ba-8d0c-b45a344af3ae",
   "metadata": {},
   "source": [
    "# Defining and Training MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdecb99c-75e4-49e2-9a80-87f0bcd54cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Mitkar\\Desktop\\LSN\\NEW-VENV\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "# Define the MLP model\n",
    "mlp_model = Sequential([\n",
    "    Flatten(input_shape=(100,)),  # Flatten the input\n",
    "    Dense(128, activation='relu'),  # First hidden layer\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Dense(64, activation='relu'),  # Second hidden layer\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer (multi-class classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "367c50f9-f806-4593-99ea-6ef7fdbd2c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.0558 - loss: 148.2003 - val_accuracy: 0.0735 - val_loss: 3.3349\n",
      "Epoch 2/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0754 - loss: 3.6240 - val_accuracy: 0.0735 - val_loss: 3.3101\n",
      "Epoch 3/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0760 - loss: 3.4040 - val_accuracy: 0.0735 - val_loss: 3.3036\n",
      "Epoch 4/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0713 - loss: 3.3521 - val_accuracy: 0.0735 - val_loss: 3.3018\n",
      "Epoch 5/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.0750 - loss: 3.3634 - val_accuracy: 0.0735 - val_loss: 3.3014\n",
      "Epoch 6/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0732 - loss: 3.3185 - val_accuracy: 0.0735 - val_loss: 3.3011\n",
      "Epoch 7/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0746 - loss: 3.3138 - val_accuracy: 0.0735 - val_loss: 3.3010\n",
      "Epoch 8/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0755 - loss: 3.3099 - val_accuracy: 0.0735 - val_loss: 3.3009\n",
      "Epoch 9/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0742 - loss: 3.3106 - val_accuracy: 0.0735 - val_loss: 3.3008\n",
      "Epoch 10/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0735 - loss: 3.3077 - val_accuracy: 0.0735 - val_loss: 3.3008\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = mlp_model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=10,  # You can adjust the number of epochs\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    batch_size=32  # Adjust based on your system's memory capacity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131b9e8-4366-4384-9625-b2cb3daa5293",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86e7a42a-26dd-414e-972d-91f5c0f9bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0693 - loss: 3.3022\n",
      "Test Loss: 3.300804615020752\n",
      "Test Accuracy: 0.07345160096883774\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loss, accuracy = mlp_model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffb24d-b2ef-4ae7-9782-d1ec465a5e3e",
   "metadata": {},
   "source": [
    "# Training Bidirectional LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c32cb-27a7-4603-8c31-0b369fd87de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Mitkar\\Desktop\\LSN\\NEW-VENV\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 264ms/step - accuracy: 0.3315 - loss: 2.2738 - val_accuracy: 0.8469 - val_loss: 0.5330\n",
      "Epoch 2/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 203ms/step - accuracy: 0.8272 - loss: 0.6096 - val_accuracy: 0.9034 - val_loss: 0.3455\n",
      "Epoch 3/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 238ms/step - accuracy: 0.8894 - loss: 0.3874 - val_accuracy: 0.8824 - val_loss: 0.3716\n",
      "Epoch 4/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 207ms/step - accuracy: 0.9093 - loss: 0.3196 - val_accuracy: 0.9195 - val_loss: 0.2760\n",
      "Epoch 5/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 190ms/step - accuracy: 0.9175 - loss: 0.2831 - val_accuracy: 0.8623 - val_loss: 0.3793\n",
      "Epoch 6/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 214ms/step - accuracy: 0.9143 - loss: 0.2799 - val_accuracy: 0.9186 - val_loss: 0.2896\n",
      "Epoch 7/10\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 227ms/step - accuracy: 0.9221 - loss: 0.2599 - val_accuracy: 0.9185 - val_loss: 0.2998\n",
      "Epoch 8/10\n",
      "\u001b[1m294/804\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 207ms/step - accuracy: 0.9233 - loss: 0.2344"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the Bidirectional LSTM model\n",
    "model = Sequential([\n",
    "    # Embedding Layer\n",
    "    Embedding(input_dim=5000,  # Vocabulary size (must match tokenizer num_words)\n",
    "              output_dim=128,  # Embedding vector size\n",
    "              input_length=100),  # Max sequence length (must match padding)\n",
    "    \n",
    "    # Bidirectional LSTM Layer\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    \n",
    "    # Additional LSTM layer for deeper learning\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Number of classes in the dataset\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388e289-d74a-4107-8d55-1917b09f9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict and evaluate classification performance\n",
    "y_pred = np.argmax(model.predict(X_test_padded), axis=1)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a0b66-46a2-412b-99eb-c44c707afd9a",
   "metadata": {},
   "source": [
    "# Testing Input on LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bbf19-2b87-4ca9-a741-410220a9e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_language(input_text, tokenizer, model, label_encoder, max_len=100):\n",
    "    \"\"\"\n",
    "    Predict the language of a given input text.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The text to classify.\n",
    "        tokenizer (Tokenizer): The fitted tokenizer used for text preprocessing.\n",
    "        model (Sequential): The trained model.\n",
    "        label_encoder (LabelEncoder): The fitted label encoder for decoding labels.\n",
    "        max_len (int): Maximum sequence length for padding (default is 100).\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted language.\n",
    "    \"\"\"\n",
    "    # Preprocess the input text\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    text_sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    text_padded = pad_sequences(text_sequence, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Predict the language\n",
    "    prediction = model.predict(text_padded)\n",
    "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_language = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "    return predicted_language\n",
    "\n",
    "# Example usage\n",
    "input_text = \" 彼はずっと世界中を旅してさまざまな文化を体験.\"  # Replace with your input text\n",
    "predicted_language = predict_language(input_text, tokenizer, model, label_encoder)\n",
    "print(f\"The predicted language is: {predicted_language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb5eec-d0e2-4464-86d2-9f736de3878f",
   "metadata": {},
   "source": [
    "# Trainiing GRU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202734-9ac5-4098-a55b-631290349b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the GRU model\n",
    "gru_model = Sequential([\n",
    "    # Embedding Layer\n",
    "    Embedding(input_dim=5000,  # Vocabulary size (must match tokenizer num_words)\n",
    "              output_dim=128,  # Embedding vector size\n",
    "              input_length=100),  # Max sequence length (must match padding)\n",
    "    \n",
    "    # Bidirectional GRU Layer\n",
    "    Bidirectional(GRU(128, return_sequences=True)),  # GRU with return_sequences for stacking\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    \n",
    "    # Additional GRU layer\n",
    "    Bidirectional(GRU(64)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Number of classes\n",
    "])\n",
    "\n",
    "# Compile the GRU model\n",
    "gru_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the GRU model summary\n",
    "gru_model.summary()\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the GRU model\n",
    "gru_history = gru_model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the GRU model on the test set\n",
    "test_loss, test_accuracy = gru_model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict and evaluate classification performance\n",
    "y_pred = np.argmax(gru_model.predict(X_test_padded), axis=1)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dad27e-20ff-4555-8807-f6c57a6e319c",
   "metadata": {},
   "source": [
    "# Testing Prediction with help of GRU  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825355b-aa9d-4e49-81ca-d6e7968a991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample input text (change this to test different languages)\n",
    "sample_text = [ \"HELLO MY NAME IS MUHAMMAD \"]  # Exampช่วยพูดช้าๆได้ไหมle in French\n",
    "\n",
    "# Step 2: Tokenize the input text\n",
    "sample_text_sequences = tokenizer.texts_to_sequences(sample_text)\n",
    "\n",
    "# Step 3: Pad the sequences to match the input length of the model\n",
    "sample_text_padded = pad_sequences(sample_text_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Use the GRU model to predict\n",
    "prediction = gru_model.predict(sample_text_padded)\n",
    "\n",
    "# Step 5: Decode the predicted label\n",
    "predicted_label_index = np.argmax(prediction, axis=1)[0]\n",
    "predicted_language = label_encoder.inverse_transform([predicted_label_index])[0]\n",
    "\n",
    "print(f\"Input text: {sample_text[0]}\")\n",
    "print(f\"Predicted language: {predicted_language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547f55f-d615-48bf-8f6a-74b1b800fdd2",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042ebc7-da5a-4bf4-8414-db157a548fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to an HDF5 file\n",
    "gru_model.save('gru_language_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d26af-5895-4f98-b64c-89d52ff3564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "\n",
    "# Save the tokenizer to a JSON file\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_json, f)\n",
    "\n",
    "print(\"Tokenizer saved to 'tokenizer.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26761cc-7393-430a-bdce-b893fd072526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # Import the pickle module\n",
    "\n",
    "# Save the label encoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Label encoder saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb797b5b-ee73-48df-9c26-c96e72c67ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSNENV",
   "language": "python",
   "name": "lsnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
